# LangChain é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

MCPStore æä¾›äº†ä¸ LangChain çš„æ·±åº¦é›†æˆï¼Œå…è®¸æ‚¨å°† MCP å·¥å…·æ— ç¼é›†æˆåˆ° LangChain çš„å·¥ä½œæµä¸­ã€‚é€šè¿‡è¿™ç§é›†æˆï¼Œæ‚¨å¯ä»¥åœ¨ LangChain çš„ Agent å’Œ Chain ä¸­ä½¿ç”¨ MCPStore ç®¡ç†çš„æ‰€æœ‰å·¥å…·ã€‚

## ğŸ—ï¸ é›†æˆæ¶æ„

```mermaid
graph TB
    subgraph "LangChain ç”Ÿæ€"
        A[LangChain Agent]
        B[LangChain Chain]
        C[LangChain Tools]
        D[LangChain Memory]
    end
    
    subgraph "MCPStore é€‚é…å±‚"
        E[LangChain Adapter]
        F[Tool Converter]
        G[Schema Mapper]
        H[Result Processor]
    end
    
    subgraph "MCPStore æ ¸å¿ƒ"
        I[MCPStore]
        J[Tool Manager]
        K[Service Manager]
        L[MCP Services]
    end
    
    A --> E
    B --> E
    C --> F
    D --> G
    
    E --> I
    F --> J
    G --> K
    H --> L
```

## ğŸ”§ åŸºç¡€é›†æˆ

### MCPStore LangChain é€‚é…å™¨

```python
from langchain.tools import BaseTool
from langchain.agents import initialize_agent, AgentType
from langchain.llms import OpenAI
from langchain.schema import AgentAction, AgentFinish
from typing import Optional, Type, Any, Dict, List
import json

class MCPStoreLangChainAdapter:
    """MCPStore LangChain é€‚é…å™¨"""
    
    def __init__(self, mcpstore):
        self.mcpstore = mcpstore
        self.langchain_tools = []
        self._convert_tools()
    
    def _convert_tools(self):
        """å°† MCPStore å·¥å…·è½¬æ¢ä¸º LangChain å·¥å…·"""
        mcp_tools = self.mcpstore.list_tools()
        
        for tool_info in mcp_tools:
            langchain_tool = self._create_langchain_tool(tool_info)
            self.langchain_tools.append(langchain_tool)
    
    def _create_langchain_tool(self, tool_info):
        """åˆ›å»º LangChain å·¥å…·"""
        
        class MCPTool(BaseTool):
            name = tool_info['name']
            description = tool_info.get('description', f"MCP tool: {tool_info['name']}")
            
            def __init__(self, mcpstore, tool_info):
                super().__init__()
                self.mcpstore = mcpstore
                self.tool_info = tool_info
            
            def _run(self, **kwargs) -> str:
                """æ‰§è¡Œå·¥å…·"""
                try:
                    # è°ƒç”¨ MCPStore å·¥å…·
                    result = self.mcpstore.call_tool(
                        self.tool_info['name'], 
                        kwargs
                    )
                    
                    # å¤„ç†ç»“æœ
                    if isinstance(result, dict):
                        return json.dumps(result, ensure_ascii=False, indent=2)
                    else:
                        return str(result)
                        
                except Exception as e:
                    return f"Error executing tool {self.name}: {str(e)}"
            
            async def _arun(self, **kwargs) -> str:
                """å¼‚æ­¥æ‰§è¡Œå·¥å…·"""
                # å¯¹äºå¼‚æ­¥æ‰§è¡Œï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹æ± 
                import asyncio
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self._run, **kwargs)
        
        return MCPTool(self.mcpstore, tool_info)
    
    def get_langchain_tools(self) -> List[BaseTool]:
        """è·å– LangChain å·¥å…·åˆ—è¡¨"""
        return self.langchain_tools
    
    def refresh_tools(self):
        """åˆ·æ–°å·¥å…·åˆ—è¡¨"""
        self.langchain_tools.clear()
        self._convert_tools()

# ä½¿ç”¨é€‚é…å™¨
from mcpstore import MCPStore

# åˆå§‹åŒ– MCPStore
store = MCPStore()
store.add_service({
    "mcpServers": {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
        }
    }
})

# åˆ›å»ºé€‚é…å™¨
adapter = MCPStoreLangChainAdapter(store)
langchain_tools = adapter.get_langchain_tools()

print(f"ğŸ”§ è½¬æ¢äº† {len(langchain_tools)} ä¸ªå·¥å…·åˆ° LangChain")
```

### LangChain Agent é›†æˆ

```python
from langchain.agents import initialize_agent, AgentType
from langchain.llms import OpenAI
from langchain.memory import ConversationBufferMemory

class MCPStoreLangChainAgent:
    """MCPStore LangChain Agent"""
    
    def __init__(self, mcpstore, llm=None, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION):
        self.mcpstore = mcpstore
        self.adapter = MCPStoreLangChainAdapter(mcpstore)
        
        # åˆå§‹åŒ– LLM
        if llm is None:
            llm = OpenAI(temperature=0)
        self.llm = llm
        
        # åˆå§‹åŒ–è®°å¿†
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # åˆ›å»º Agent
        self.agent = initialize_agent(
            tools=self.adapter.get_langchain_tools(),
            llm=self.llm,
            agent=agent_type,
            memory=self.memory,
            verbose=True,
            handle_parsing_errors=True
        )
    
    def run(self, query: str) -> str:
        """è¿è¡Œ Agent"""
        try:
            result = self.agent.run(query)
            return result
        except Exception as e:
            return f"Agent execution failed: {str(e)}"
    
    def add_custom_tool(self, tool_name: str, tool_func, description: str):
        """æ·»åŠ è‡ªå®šä¹‰å·¥å…·"""
        
        class CustomTool(BaseTool):
            name = tool_name
            description = description
            
            def _run(self, **kwargs) -> str:
                try:
                    result = tool_func(**kwargs)
                    return str(result)
                except Exception as e:
                    return f"Error: {str(e)}"
            
            async def _arun(self, **kwargs) -> str:
                return self._run(**kwargs)
        
        # æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨
        custom_tool = CustomTool()
        self.agent.tools.append(custom_tool)
        
        print(f"âœ… æ·»åŠ è‡ªå®šä¹‰å·¥å…·: {tool_name}")
    
    def refresh_tools(self):
        """åˆ·æ–°å·¥å…·"""
        self.adapter.refresh_tools()
        
        # é‡æ–°åˆå§‹åŒ– Agent
        self.agent = initialize_agent(
            tools=self.adapter.get_langchain_tools(),
            llm=self.llm,
            agent=self.agent.agent_type,
            memory=self.memory,
            verbose=True
        )

# ä½¿ç”¨ MCPStore LangChain Agent
agent = MCPStoreLangChainAgent(store)

# æµ‹è¯• Agent
queries = [
    "åˆ—å‡º /tmp ç›®å½•ä¸‹çš„æ–‡ä»¶",
    "åˆ›å»ºä¸€ä¸ªåä¸º test.txt çš„æ–‡ä»¶ï¼Œå†…å®¹æ˜¯ 'Hello LangChain'",
    "è¯»å–åˆšæ‰åˆ›å»ºçš„ test.txt æ–‡ä»¶çš„å†…å®¹"
]

for query in queries:
    print(f"\nğŸ¤– æŸ¥è¯¢: {query}")
    result = agent.run(query)
    print(f"ğŸ“ ç»“æœ: {result}")
```

## ğŸ”— é«˜çº§é›†æˆåŠŸèƒ½

### è‡ªå®šä¹‰ LangChain Chain

```python
from langchain.chains.base import Chain
from langchain.schema import BasePromptTemplate
from langchain.prompts import PromptTemplate
from typing import Dict, List

class MCPStoreChain(Chain):
    """MCPStore è‡ªå®šä¹‰é“¾"""
    
    mcpstore: Any
    prompt: BasePromptTemplate
    llm: Any
    output_key: str = "result"
    
    class Config:
        arbitrary_types_allowed = True
    
    @property
    def input_keys(self) -> List[str]:
        """è¾“å…¥é”®"""
        return ["task", "context"]
    
    @property
    def output_keys(self) -> List[str]:
        """è¾“å‡ºé”®"""
        return [self.output_key]
    
    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé“¾"""
        task = inputs["task"]
        context = inputs.get("context", {})
        
        # 1. åˆ†æä»»åŠ¡ï¼Œç¡®å®šéœ€è¦çš„å·¥å…·
        tools_needed = self._analyze_task(task)
        
        # 2. æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_results = self._execute_tools(tools_needed, context)
        
        # 3. ä½¿ç”¨ LLM å¤„ç†ç»“æœ
        final_result = self._process_results(task, tool_results)
        
        return {self.output_key: final_result}
    
    def _analyze_task(self, task: str) -> List[Dict]:
        """åˆ†æä»»åŠ¡ï¼Œç¡®å®šéœ€è¦çš„å·¥å…·"""
        # ä½¿ç”¨ LLM åˆ†æä»»åŠ¡
        analysis_prompt = PromptTemplate(
            input_variables=["task", "available_tools"],
            template="""
            ä»»åŠ¡: {task}
            
            å¯ç”¨å·¥å…·: {available_tools}
            
            è¯·åˆ†æè¿™ä¸ªä»»åŠ¡éœ€è¦ä½¿ç”¨å“ªäº›å·¥å…·ï¼Œä»¥ä»€ä¹ˆé¡ºåºæ‰§è¡Œï¼Œéœ€è¦ä»€ä¹ˆå‚æ•°ã€‚
            è¿”å› JSON æ ¼å¼çš„å·¥å…·è°ƒç”¨è®¡åˆ’ã€‚
            """
        )
        
        # è·å–å¯ç”¨å·¥å…·
        available_tools = [tool['name'] for tool in self.mcpstore.list_tools()]
        
        # ç”Ÿæˆåˆ†æ
        analysis_input = analysis_prompt.format(
            task=task,
            available_tools=", ".join(available_tools)
        )
        
        analysis_result = self.llm(analysis_input)
        
        # è§£æåˆ†æç»“æœ
        try:
            import json
            tools_plan = json.loads(analysis_result)
            return tools_plan
        except:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºè®¡åˆ’
            return []
    
    def _execute_tools(self, tools_plan: List[Dict], context: Dict) -> List[Dict]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        results = []
        
        for tool_call in tools_plan:
            try:
                tool_name = tool_call.get('tool_name')
                arguments = tool_call.get('arguments', {})
                
                # æ›¿æ¢ä¸Šä¸‹æ–‡å˜é‡
                arguments = self._substitute_context(arguments, context)
                
                # æ‰§è¡Œå·¥å…·
                result = self.mcpstore.call_tool(tool_name, arguments)
                
                results.append({
                    'tool_name': tool_name,
                    'arguments': arguments,
                    'result': result,
                    'success': True
                })
                
                # æ›´æ–°ä¸Šä¸‹æ–‡
                context[f"{tool_name}_result"] = result
                
            except Exception as e:
                results.append({
                    'tool_name': tool_call.get('tool_name'),
                    'arguments': tool_call.get('arguments', {}),
                    'error': str(e),
                    'success': False
                })
        
        return results
    
    def _substitute_context(self, arguments: Dict, context: Dict) -> Dict:
        """æ›¿æ¢ä¸Šä¸‹æ–‡å˜é‡"""
        import re
        
        def replace_vars(obj):
            if isinstance(obj, str):
                # æ›¿æ¢ ${variable} æ ¼å¼çš„å˜é‡
                pattern = r'\$\{([^}]+)\}'
                
                def replacer(match):
                    var_name = match.group(1)
                    return str(context.get(var_name, match.group(0)))
                
                return re.sub(pattern, replacer, obj)
            elif isinstance(obj, dict):
                return {k: replace_vars(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [replace_vars(item) for item in obj]
            else:
                return obj
        
        return replace_vars(arguments)
    
    def _process_results(self, task: str, tool_results: List[Dict]) -> str:
        """å¤„ç†å·¥å…·ç»“æœ"""
        # æ„é€ ç»“æœå¤„ç†æç¤º
        results_prompt = PromptTemplate(
            input_variables=["task", "tool_results"],
            template="""
            åŸå§‹ä»»åŠ¡: {task}
            
            å·¥å…·æ‰§è¡Œç»“æœ: {tool_results}
            
            è¯·æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœï¼Œç”Ÿæˆå¯¹åŸå§‹ä»»åŠ¡çš„å®Œæ•´å›ç­”ã€‚
            """
        )
        
        # æ ¼å¼åŒ–å·¥å…·ç»“æœ
        formatted_results = []
        for result in tool_results:
            if result['success']:
                formatted_results.append(f"å·¥å…· {result['tool_name']}: æˆåŠŸ - {result['result']}")
            else:
                formatted_results.append(f"å·¥å…· {result['tool_name']}: å¤±è´¥ - {result['error']}")
        
        # ç”Ÿæˆæœ€ç»ˆç»“æœ
        final_input = results_prompt.format(
            task=task,
            tool_results="\n".join(formatted_results)
        )
        
        return self.llm(final_input)

# ä½¿ç”¨è‡ªå®šä¹‰é“¾
from langchain.llms import OpenAI

custom_chain = MCPStoreChain(
    mcpstore=store,
    llm=OpenAI(temperature=0),
    prompt=PromptTemplate(input_variables=["task"], template="{task}")
)

# æµ‹è¯•è‡ªå®šä¹‰é“¾
result = custom_chain({
    "task": "åˆ›å»ºä¸€ä¸ªæŠ¥å‘Šæ–‡ä»¶ï¼ŒåŒ…å«å½“å‰ç›®å½•çš„æ–‡ä»¶åˆ—è¡¨",
    "context": {"output_dir": "/tmp"}
})

print(f"ğŸ”— è‡ªå®šä¹‰é“¾ç»“æœ: {result['result']}")
```

### å·¥å…·ç»„åˆå’Œå·¥ä½œæµ

```python
from langchain.chains import SequentialChain
from langchain.chains.llm import LLMChain

class MCPStoreWorkflow:
    """MCPStore å·¥ä½œæµ"""
    
    def __init__(self, mcpstore, llm):
        self.mcpstore = mcpstore
        self.llm = llm
        self.workflows = {}
    
    def create_workflow(self, name: str, steps: List[Dict]):
        """åˆ›å»ºå·¥ä½œæµ"""
        chains = []
        
        for i, step in enumerate(steps):
            step_name = f"step_{i+1}"
            
            if step['type'] == 'tool_call':
                # å·¥å…·è°ƒç”¨æ­¥éª¤
                chain = self._create_tool_chain(step_name, step)
            elif step['type'] == 'llm_process':
                # LLM å¤„ç†æ­¥éª¤
                chain = self._create_llm_chain(step_name, step)
            else:
                raise ValueError(f"Unknown step type: {step['type']}")
            
            chains.append(chain)
        
        # åˆ›å»ºé¡ºåºé“¾
        workflow = SequentialChain(
            chains=chains,
            input_variables=["input"],
            output_variables=[f"step_{len(steps)}_output"],
            verbose=True
        )
        
        self.workflows[name] = workflow
        return workflow
    
    def _create_tool_chain(self, step_name: str, step_config: Dict):
        """åˆ›å»ºå·¥å…·è°ƒç”¨é“¾"""
        
        class ToolCallChain(Chain):
            mcpstore: Any
            tool_name: str
            arguments_template: Dict
            
            @property
            def input_keys(self) -> List[str]:
                return ["input"]
            
            @property
            def output_keys(self) -> List[str]:
                return [f"{step_name}_output"]
            
            def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
                # å¤„ç†å‚æ•°æ¨¡æ¿
                arguments = self._process_arguments(inputs)
                
                # è°ƒç”¨å·¥å…·
                result = self.mcpstore.call_tool(self.tool_name, arguments)
                
                return {f"{step_name}_output": result}
            
            def _process_arguments(self, inputs: Dict) -> Dict:
                """å¤„ç†å‚æ•°æ¨¡æ¿"""
                import re
                
                def substitute_vars(obj):
                    if isinstance(obj, str):
                        # æ›¿æ¢å˜é‡
                        for key, value in inputs.items():
                            obj = obj.replace(f"{{{key}}}", str(value))
                        return obj
                    elif isinstance(obj, dict):
                        return {k: substitute_vars(v) for k, v in obj.items()}
                    else:
                        return obj
                
                return substitute_vars(self.arguments_template)
        
        return ToolCallChain(
            mcpstore=self.mcpstore,
            tool_name=step_config['tool_name'],
            arguments_template=step_config.get('arguments', {})
        )
    
    def _create_llm_chain(self, step_name: str, step_config: Dict):
        """åˆ›å»º LLM å¤„ç†é“¾"""
        prompt = PromptTemplate(
            input_variables=step_config.get('input_variables', ['input']),
            template=step_config['prompt_template']
        )
        
        return LLMChain(
            llm=self.llm,
            prompt=prompt,
            output_key=f"{step_name}_output"
        )
    
    def run_workflow(self, workflow_name: str, input_data: Dict) -> Dict:
        """è¿è¡Œå·¥ä½œæµ"""
        if workflow_name not in self.workflows:
            raise ValueError(f"Workflow {workflow_name} not found")
        
        workflow = self.workflows[workflow_name]
        return workflow(input_data)

# åˆ›å»ºå·¥ä½œæµ
workflow_manager = MCPStoreWorkflow(store, OpenAI(temperature=0))

# å®šä¹‰æ–‡ä»¶å¤„ç†å·¥ä½œæµ
file_processing_steps = [
    {
        'type': 'tool_call',
        'tool_name': 'read_file',
        'arguments': {'path': '{file_path}'}
    },
    {
        'type': 'llm_process',
        'prompt_template': 'åˆ†æä»¥ä¸‹æ–‡ä»¶å†…å®¹å¹¶ç”Ÿæˆæ‘˜è¦:\n\n{step_1_output}\n\næ‘˜è¦:',
        'input_variables': ['step_1_output']
    },
    {
        'type': 'tool_call',
        'tool_name': 'write_file',
        'arguments': {
            'path': '{output_path}',
            'content': '{step_2_output}'
        }
    }
]

# åˆ›å»ºå·¥ä½œæµ
workflow = workflow_manager.create_workflow('file_processing', file_processing_steps)

# è¿è¡Œå·¥ä½œæµ
result = workflow_manager.run_workflow('file_processing', {
    'input': 'process file',
    'file_path': '/tmp/input.txt',
    'output_path': '/tmp/summary.txt'
})

print(f"ğŸ”„ å·¥ä½œæµç»“æœ: {result}")
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å·¥å…·ç®¡ç†ç³»ç»Ÿ](../tools/management/tool-management.md)
- [é“¾å¼è°ƒç”¨æœºåˆ¶](chaining.md)
- [FastMCP é›†æˆ](fastmcp-integration.md)
- [å®Œæ•´ç¤ºä¾‹é›†åˆ](../examples/complete-examples.md)

## ğŸ“š é›†æˆæœ€ä½³å®è·µ

1. **å·¥å…·è½¬æ¢**ï¼šç¡®ä¿ MCP å·¥å…·æ­£ç¡®è½¬æ¢ä¸º LangChain å·¥å…·æ ¼å¼
2. **é”™è¯¯å¤„ç†**ï¼šåœ¨ LangChain é›†æˆä¸­å®ç°å®Œå–„çš„é”™è¯¯å¤„ç†
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šä½¿ç”¨æ‰¹é‡è°ƒç”¨å’Œç¼“å­˜æé«˜æ€§èƒ½
4. **å·¥ä½œæµè®¾è®¡**ï¼šåˆç†è®¾è®¡å·¥å…·ç»„åˆå’Œæ‰§è¡Œé¡ºåº
5. **çŠ¶æ€ç®¡ç†**ï¼šæ­£ç¡®ç®¡ç†å·¥ä½œæµä¸­çš„çŠ¶æ€å’Œä¸Šä¸‹æ–‡
6. **ç›‘æ§æ—¥å¿—**ï¼šè®°å½• LangChain é›†æˆçš„æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœ

---

**æ›´æ–°æ—¶é—´**: 2025-01-09  
**ç‰ˆæœ¬**: 1.0.0
