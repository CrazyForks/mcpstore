# æ€§èƒ½ä¼˜åŒ–æ·±åº¦æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº† MCPStore æ€§èƒ½ä¼˜åŒ–çš„æ·±åº¦æŒ‡å—ï¼Œæ¶µç›–ç³»ç»Ÿçº§ä¼˜åŒ–ã€åº”ç”¨çº§ä¼˜åŒ–ã€ç½‘ç»œä¼˜åŒ–ç­‰å¤šä¸ªå±‚é¢çš„ä¼˜åŒ–ç­–ç•¥å’Œå®è·µæ–¹æ³•ã€‚

## ğŸ—ï¸ æ€§èƒ½ä¼˜åŒ–ä½“ç³»

```mermaid
graph TB
    A[æ€§èƒ½ä¼˜åŒ–ä½“ç³»] --> B[ç³»ç»Ÿçº§ä¼˜åŒ–]
    A --> C[åº”ç”¨çº§ä¼˜åŒ–]
    A --> D[ç½‘ç»œä¼˜åŒ–]
    A --> E[å­˜å‚¨ä¼˜åŒ–]
    
    B --> F[CPUä¼˜åŒ–]
    B --> G[å†…å­˜ä¼˜åŒ–]
    B --> H[I/Oä¼˜åŒ–]
    
    C --> I[ç®—æ³•ä¼˜åŒ–]
    C --> J[ç¼“å­˜ç­–ç•¥]
    C --> K[å¹¶å‘ä¼˜åŒ–]
    
    D --> L[è¿æ¥ä¼˜åŒ–]
    D --> M[åè®®ä¼˜åŒ–]
    D --> N[å¸¦å®½ä¼˜åŒ–]
    
    E --> O[æ•°æ®ç»“æ„]
    E --> P[åºåˆ—åŒ–]
    E --> Q[å‹ç¼©ç®—æ³•]
```

## ğŸš€ ç³»ç»Ÿçº§æ€§èƒ½ä¼˜åŒ–

### CPU ä¼˜åŒ–ç­–ç•¥

```python
import multiprocessing
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import asyncio

class CPUOptimizer:
    """CPUä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.cpu_count = multiprocessing.cpu_count()
        self.thread_pool = None
        self.process_pool = None
        self.async_semaphore = None
    
    def optimize_thread_pool(self, max_workers=None):
        """ä¼˜åŒ–çº¿ç¨‹æ± é…ç½®"""
        if max_workers is None:
            # I/Oå¯†é›†å‹ä»»åŠ¡ï¼šCPUæ ¸å¿ƒæ•° * 2-4
            max_workers = self.cpu_count * 3
        
        self.thread_pool = ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix="mcpstore-worker"
        )
        
        print(f"ğŸ”§ çº¿ç¨‹æ± ä¼˜åŒ–: {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        return self.thread_pool
    
    def optimize_process_pool(self, max_workers=None):
        """ä¼˜åŒ–è¿›ç¨‹æ± é…ç½®"""
        if max_workers is None:
            # CPUå¯†é›†å‹ä»»åŠ¡ï¼šCPUæ ¸å¿ƒæ•°
            max_workers = self.cpu_count
        
        self.process_pool = ProcessPoolExecutor(
            max_workers=max_workers
        )
        
        print(f"ğŸ”§ è¿›ç¨‹æ± ä¼˜åŒ–: {max_workers} ä¸ªå·¥ä½œè¿›ç¨‹")
        return self.process_pool
    
    def optimize_async_concurrency(self, max_concurrent=None):
        """ä¼˜åŒ–å¼‚æ­¥å¹¶å‘æ•°"""
        if max_concurrent is None:
            # å¼‚æ­¥ä»»åŠ¡ï¼šCPUæ ¸å¿ƒæ•° * 10-50
            max_concurrent = self.cpu_count * 20
        
        self.async_semaphore = asyncio.Semaphore(max_concurrent)
        
        print(f"ğŸ”§ å¼‚æ­¥å¹¶å‘ä¼˜åŒ–: {max_concurrent} ä¸ªå¹¶å‘ä»»åŠ¡")
        return self.async_semaphore
    
    def cpu_intensive_task_optimizer(self, task_func, data_chunks):
        """CPUå¯†é›†å‹ä»»åŠ¡ä¼˜åŒ–"""
        with self.process_pool as executor:
            futures = [executor.submit(task_func, chunk) for chunk in data_chunks]
            results = [future.result() for future in futures]
        
        return results
    
    def io_intensive_task_optimizer(self, task_func, task_args_list):
        """I/Oå¯†é›†å‹ä»»åŠ¡ä¼˜åŒ–"""
        with self.thread_pool as executor:
            futures = [executor.submit(task_func, *args) for args in task_args_list]
            results = [future.result() for future in futures]
        
        return results

# ä½¿ç”¨CPUä¼˜åŒ–å™¨
cpu_optimizer = CPUOptimizer()
thread_pool = cpu_optimizer.optimize_thread_pool()
process_pool = cpu_optimizer.optimize_process_pool()
```

### å†…å­˜ä¼˜åŒ–ç­–ç•¥

```python
import gc
import sys
import weakref
from collections import deque
import psutil

class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.memory_threshold = 0.8  # 80%å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
        self.gc_threshold = (700, 10, 10)  # åƒåœ¾å›æ”¶é˜ˆå€¼
        self.object_pools = {}
        self.weak_references = weakref.WeakValueDictionary()
    
    def optimize_garbage_collection(self):
        """ä¼˜åŒ–åƒåœ¾å›æ”¶"""
        # è®¾ç½®åƒåœ¾å›æ”¶é˜ˆå€¼
        gc.set_threshold(*self.gc_threshold)
        
        # å¯ç”¨åƒåœ¾å›æ”¶è°ƒè¯•
        # gc.set_debug(gc.DEBUG_STATS)
        
        print(f"ğŸ—‘ï¸ åƒåœ¾å›æ”¶ä¼˜åŒ–: é˜ˆå€¼ {self.gc_threshold}")
    
    def create_object_pool(self, name, factory, max_size=100):
        """åˆ›å»ºå¯¹è±¡æ± """
        self.object_pools[name] = ObjectPool(factory, max_size)
        print(f"ğŸŠ å¯¹è±¡æ± åˆ›å»º: {name} (æœ€å¤§ {max_size} ä¸ªå¯¹è±¡)")
    
    def get_from_pool(self, pool_name):
        """ä»å¯¹è±¡æ± è·å–å¯¹è±¡"""
        pool = self.object_pools.get(pool_name)
        if pool:
            return pool.get()
        return None
    
    def return_to_pool(self, pool_name, obj):
        """è¿”å›å¯¹è±¡åˆ°æ± """
        pool = self.object_pools.get(pool_name)
        if pool:
            pool.put(obj)
    
    def monitor_memory_usage(self):
        """ç›‘æ§å†…å­˜ä½¿ç”¨"""
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_percent = process.memory_percent()
        
        if memory_percent > self.memory_threshold * 100:
            print(f"âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1f}%")
            self.trigger_memory_cleanup()
        
        return {
            'rss': memory_info.rss,
            'vms': memory_info.vms,
            'percent': memory_percent
        }
    
    def trigger_memory_cleanup(self):
        """è§¦å‘å†…å­˜æ¸…ç†"""
        print("ğŸ§¹ å¼€å§‹å†…å­˜æ¸…ç†...")
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        print(f"   åƒåœ¾å›æ”¶: æ¸…ç† {collected} ä¸ªå¯¹è±¡")
        
        # æ¸…ç†å¯¹è±¡æ± 
        for name, pool in self.object_pools.items():
            cleaned = pool.cleanup()
            print(f"   å¯¹è±¡æ±  {name}: æ¸…ç† {cleaned} ä¸ªå¯¹è±¡")
        
        # æ¸…ç†å¼±å¼•ç”¨
        self.weak_references.clear()
        print("   å¼±å¼•ç”¨: å·²æ¸…ç†")

class ObjectPool:
    """å¯¹è±¡æ± å®ç°"""
    
    def __init__(self, factory, max_size=100):
        self.factory = factory
        self.max_size = max_size
        self.pool = deque()
        self.created_count = 0
        self.reused_count = 0
    
    def get(self):
        """è·å–å¯¹è±¡"""
        if self.pool:
            obj = self.pool.popleft()
            self.reused_count += 1
            return obj
        else:
            obj = self.factory()
            self.created_count += 1
            return obj
    
    def put(self, obj):
        """æ”¾å›å¯¹è±¡"""
        if len(self.pool) < self.max_size:
            # é‡ç½®å¯¹è±¡çŠ¶æ€
            if hasattr(obj, 'reset'):
                obj.reset()
            self.pool.append(obj)
    
    def cleanup(self):
        """æ¸…ç†æ± """
        cleaned = len(self.pool)
        self.pool.clear()
        return cleaned
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'pool_size': len(self.pool),
            'created_count': self.created_count,
            'reused_count': self.reused_count,
            'reuse_rate': self.reused_count / (self.created_count + self.reused_count) if (self.created_count + self.reused_count) > 0 else 0
        }

# ä½¿ç”¨å†…å­˜ä¼˜åŒ–å™¨
memory_optimizer = MemoryOptimizer()
memory_optimizer.optimize_garbage_collection()

# åˆ›å»ºå¯¹è±¡æ± 
def create_result_object():
    return {'data': None, 'status': 'ready'}

memory_optimizer.create_object_pool('results', create_result_object, max_size=50)
```

## âš¡ åº”ç”¨çº§æ€§èƒ½ä¼˜åŒ–

### æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

```python
import time
import hashlib
import pickle
from functools import wraps
from typing import Any, Optional, Callable

class IntelligentCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, max_size=1000, default_ttl=300):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache = {}
        self.access_times = {}
        self.hit_count = 0
        self.miss_count = 0
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        if key in self.cache:
            value, expiry = self.cache[key]
            
            if time.time() < expiry:
                self.access_times[key] = time.time()
                self.hit_count += 1
                return value
            else:
                # ç¼“å­˜è¿‡æœŸ
                del self.cache[key]
                del self.access_times[key]
        
        self.miss_count += 1
        return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """è®¾ç½®ç¼“å­˜"""
        if ttl is None:
            ttl = self.default_ttl
        
        expiry = time.time() + ttl
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°
        if len(self.cache) >= self.max_size:
            self._evict_lru()
        
        self.cache[key] = (value, expiry)
        self.access_times[key] = time.time()
    
    def _evict_lru(self):
        """LRUæ·˜æ±°ç­–ç•¥"""
        if not self.access_times:
            return
        
        # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„é”®
        lru_key = min(self.access_times, key=self.access_times.get)
        
        # åˆ é™¤ç¼“å­˜é¡¹
        del self.cache[lru_key]
        del self.access_times[lru_key]
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_times.clear()
    
    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
        
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': hit_rate,
            'usage_rate': len(self.cache) / self.max_size
        }

def cache_result(cache: IntelligentCache, ttl: Optional[int] = None, key_func: Optional[Callable] = None):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                cache_key = _generate_cache_key(func.__name__, args, kwargs)
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            cache.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator

def _generate_cache_key(func_name: str, args: tuple, kwargs: dict) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    key_data = {
        'func': func_name,
        'args': args,
        'kwargs': kwargs
    }
    
    # åºåˆ—åŒ–å¹¶ç”Ÿæˆå“ˆå¸Œ
    serialized = pickle.dumps(key_data, protocol=pickle.HIGHEST_PROTOCOL)
    return hashlib.md5(serialized).hexdigest()

# ä½¿ç”¨æ™ºèƒ½ç¼“å­˜
intelligent_cache = IntelligentCache(max_size=500, default_ttl=600)

@cache_result(intelligent_cache, ttl=300)
def expensive_operation(param1, param2):
    """æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ"""
    time.sleep(1)  # æ¨¡æ‹Ÿè€—æ—¶
    return f"Result for {param1} and {param2}"

# æµ‹è¯•ç¼“å­˜æ•ˆæœ
start_time = time.time()
result1 = expensive_operation("test", "data")  # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œä¼šç¼“å­˜
result2 = expensive_operation("test", "data")  # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œä»ç¼“å­˜è·å–
end_time = time.time()

print(f"âš¡ ç¼“å­˜æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}s")
print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {intelligent_cache.get_stats()}")
```

### æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
class BatchOptimizer:
    """æ‰¹é‡æ“ä½œä¼˜åŒ–å™¨"""
    
    def __init__(self, store):
        self.store = store
        self.batch_size = 20
        self.max_concurrent = 5
        self.operation_queue = []
    
    def add_operation(self, operation_type, tool_name, arguments):
        """æ·»åŠ æ“ä½œåˆ°é˜Ÿåˆ—"""
        self.operation_queue.append({
            'type': operation_type,
            'tool_name': tool_name,
            'arguments': arguments,
            'timestamp': time.time()
        })
    
    def optimize_batch_execution(self):
        """ä¼˜åŒ–æ‰¹é‡æ‰§è¡Œ"""
        if not self.operation_queue:
            return []
        
        # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
        grouped_operations = self._group_operations()
        
        # ä¼˜åŒ–æ‰§è¡Œé¡ºåº
        optimized_groups = self._optimize_execution_order(grouped_operations)
        
        # å¹¶è¡Œæ‰§è¡Œ
        results = self._execute_parallel_batches(optimized_groups)
        
        # æ¸…ç©ºé˜Ÿåˆ—
        self.operation_queue.clear()
        
        return results
    
    def _group_operations(self):
        """æŒ‰æ“ä½œç±»å‹åˆ†ç»„"""
        groups = {}
        
        for operation in self.operation_queue:
            op_type = operation['type']
            if op_type not in groups:
                groups[op_type] = []
            groups[op_type].append(operation)
        
        return groups
    
    def _optimize_execution_order(self, grouped_operations):
        """ä¼˜åŒ–æ‰§è¡Œé¡ºåº"""
        # å®šä¹‰æ“ä½œä¼˜å…ˆçº§
        priority_order = ['read', 'write', 'delete', 'create']
        
        optimized_groups = []
        for op_type in priority_order:
            if op_type in grouped_operations:
                # æŒ‰æ‰¹æ¬¡å¤§å°åˆ†å‰²
                operations = grouped_operations[op_type]
                for i in range(0, len(operations), self.batch_size):
                    batch = operations[i:i + self.batch_size]
                    optimized_groups.append((op_type, batch))
        
        return optimized_groups
    
    def _execute_parallel_batches(self, optimized_groups):
        """å¹¶è¡Œæ‰§è¡Œæ‰¹æ¬¡"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        all_results = []
        
        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            # æäº¤æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {}
            for op_type, batch in optimized_groups:
                future = executor.submit(self._execute_batch, op_type, batch)
                future_to_batch[future] = (op_type, batch)
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                op_type, batch = future_to_batch[future]
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                    print(f"âœ… æ‰¹æ¬¡å®Œæˆ: {op_type} ({len(batch)} ä¸ªæ“ä½œ)")
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡å¤±è´¥: {op_type} - {e}")
        
        return all_results
    
    def _execute_batch(self, op_type, batch):
        """æ‰§è¡Œå•ä¸ªæ‰¹æ¬¡"""
        batch_calls = []
        
        for operation in batch:
            batch_calls.append({
                'tool_name': operation['tool_name'],
                'arguments': operation['arguments']
            })
        
        # æ‰§è¡Œæ‰¹é‡è°ƒç”¨
        return self.store.batch_call(batch_calls)

# ä½¿ç”¨æ‰¹é‡ä¼˜åŒ–å™¨
batch_optimizer = BatchOptimizer(store)

# æ·»åŠ å¤šä¸ªæ“ä½œ
for i in range(50):
    batch_optimizer.add_operation('read', 'read_file', {'path': f'/tmp/file_{i}.txt'})
    batch_optimizer.add_operation('write', 'write_file', {
        'path': f'/tmp/output_{i}.txt',
        'content': f'Content {i}'
    })

# ä¼˜åŒ–æ‰§è¡Œ
start_time = time.time()
results = batch_optimizer.optimize_batch_execution()
execution_time = time.time() - start_time

print(f"âš¡ æ‰¹é‡ä¼˜åŒ–å®Œæˆ: {len(results)} ä¸ªæ“ä½œï¼Œè€—æ—¶ {execution_time:.2f}s")
```

## ğŸŒ ç½‘ç»œæ€§èƒ½ä¼˜åŒ–

### è¿æ¥æ± ä¼˜åŒ–

```python
import queue
import threading
import time
from contextlib import contextmanager

class OptimizedConnectionPool:
    """ä¼˜åŒ–çš„è¿æ¥æ± """
    
    def __init__(self, service_config, min_size=2, max_size=10, max_idle_time=300):
        self.service_config = service_config
        self.min_size = min_size
        self.max_size = max_size
        self.max_idle_time = max_idle_time
        
        # è¿æ¥ç®¡ç†
        self.active_connections = set()
        self.idle_connections = queue.Queue()
        self.connection_count = 0
        self.lock = threading.RLock()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'created': 0,
            'reused': 0,
            'closed': 0,
            'timeouts': 0
        }
        
        # åˆå§‹åŒ–æœ€å°è¿æ¥æ•°
        self._initialize_pool()
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self.cleanup_thread = threading.Thread(target=self._cleanup_idle_connections, daemon=True)
        self.cleanup_thread.start()
    
    def _initialize_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        for _ in range(self.min_size):
            try:
                connection = self._create_connection()
                self.idle_connections.put((connection, time.time()))
            except Exception as e:
                print(f"âš ï¸ åˆå§‹åŒ–è¿æ¥å¤±è´¥: {e}")
    
    def _create_connection(self):
        """åˆ›å»ºæ–°è¿æ¥"""
        with self.lock:
            if self.connection_count >= self.max_size:
                raise Exception("è¿æ¥æ± å·²æ»¡")
            
            # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„è¿æ¥åˆ›å»ºé€»è¾‘
            connection = MockConnection(self.service_config)
            self.connection_count += 1
            self.stats['created'] += 1
            
            return connection
    
    @contextmanager
    def get_connection(self, timeout=30):
        """è·å–è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        connection = None
        start_time = time.time()
        
        try:
            # å°è¯•ä»ç©ºé—²è¿æ¥è·å–
            try:
                connection, _ = self.idle_connections.get_nowait()
                self.stats['reused'] += 1
            except queue.Empty:
                # åˆ›å»ºæ–°è¿æ¥
                if self.connection_count < self.max_size:
                    connection = self._create_connection()
                else:
                    # ç­‰å¾…è¿æ¥é‡Šæ”¾
                    try:
                        connection, _ = self.idle_connections.get(timeout=timeout)
                        self.stats['reused'] += 1
                    except queue.Empty:
                        self.stats['timeouts'] += 1
                        raise Exception("è·å–è¿æ¥è¶…æ—¶")
            
            # éªŒè¯è¿æ¥æœ‰æ•ˆæ€§
            if not self._validate_connection(connection):
                connection = self._create_connection()
            
            # æ·»åŠ åˆ°æ´»è·ƒè¿æ¥
            with self.lock:
                self.active_connections.add(connection)
            
            yield connection
            
        finally:
            # é‡Šæ”¾è¿æ¥
            if connection:
                self._release_connection(connection)
    
    def _release_connection(self, connection):
        """é‡Šæ”¾è¿æ¥"""
        with self.lock:
            if connection in self.active_connections:
                self.active_connections.remove(connection)
                
                # éªŒè¯è¿æ¥çŠ¶æ€
                if self._validate_connection(connection):
                    # è¿”å›ç©ºé—²æ± 
                    self.idle_connections.put((connection, time.time()))
                else:
                    # å…³é—­æ— æ•ˆè¿æ¥
                    self._close_connection(connection)
    
    def _validate_connection(self, connection):
        """éªŒè¯è¿æ¥æœ‰æ•ˆæ€§"""
        try:
            return connection.is_alive()
        except:
            return False
    
    def _close_connection(self, connection):
        """å…³é—­è¿æ¥"""
        try:
            connection.close()
            with self.lock:
                self.connection_count -= 1
                self.stats['closed'] += 1
        except:
            pass
    
    def _cleanup_idle_connections(self):
        """æ¸…ç†ç©ºé—²è¿æ¥"""
        while True:
            try:
                current_time = time.time()
                connections_to_close = []
                
                # æ£€æŸ¥ç©ºé—²è¿æ¥
                temp_connections = []
                while not self.idle_connections.empty():
                    try:
                        connection, idle_time = self.idle_connections.get_nowait()
                        
                        if current_time - idle_time > self.max_idle_time:
                            connections_to_close.append(connection)
                        else:
                            temp_connections.append((connection, idle_time))
                    except queue.Empty:
                        break
                
                # é‡æ–°æ”¾å›æœªè¿‡æœŸçš„è¿æ¥
                for conn_info in temp_connections:
                    self.idle_connections.put(conn_info)
                
                # å…³é—­è¿‡æœŸè¿æ¥
                for connection in connections_to_close:
                    self._close_connection(connection)
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                
            except Exception as e:
                print(f"âš ï¸ è¿æ¥æ¸…ç†å¤±è´¥: {e}")
                time.sleep(60)
    
    def get_stats(self):
        """è·å–è¿æ¥æ± ç»Ÿè®¡"""
        with self.lock:
            return {
                'active_connections': len(self.active_connections),
                'idle_connections': self.idle_connections.qsize(),
                'total_connections': self.connection_count,
                'max_size': self.max_size,
                'min_size': self.min_size,
                'usage_rate': self.connection_count / self.max_size,
                **self.stats
            }

class MockConnection:
    """æ¨¡æ‹Ÿè¿æ¥ç±»"""
    
    def __init__(self, config):
        self.config = config
        self.created_time = time.time()
        self.alive = True
    
    def is_alive(self):
        """æ£€æŸ¥è¿æ¥æ˜¯å¦å­˜æ´»"""
        return self.alive
    
    def close(self):
        """å…³é—­è¿æ¥"""
        self.alive = False

# ä½¿ç”¨ä¼˜åŒ–çš„è¿æ¥æ± 
pool = OptimizedConnectionPool(
    service_config={'host': 'localhost', 'port': 8080},
    min_size=3,
    max_size=15,
    max_idle_time=600
)

# æµ‹è¯•è¿æ¥æ± æ€§èƒ½
def test_connection_pool_performance():
    """æµ‹è¯•è¿æ¥æ± æ€§èƒ½"""
    start_time = time.time()
    
    # æ¨¡æ‹Ÿå¹¶å‘è¿æ¥ä½¿ç”¨
    def use_connection(pool, operation_id):
        try:
            with pool.get_connection() as conn:
                # æ¨¡æ‹Ÿæ“ä½œ
                time.sleep(0.1)
                return f"Operation {operation_id} completed"
        except Exception as e:
            return f"Operation {operation_id} failed: {e}"
    
    # å¹¶å‘æµ‹è¯•
    from concurrent.futures import ThreadPoolExecutor
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(use_connection, pool, i) for i in range(100)]
        results = [future.result() for future in futures]
    
    end_time = time.time()
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for r in results if "completed" in r)
    failed = len(results) - successful
    
    print(f"âš¡ è¿æ¥æ± æ€§èƒ½æµ‹è¯•å®Œæˆ:")
    print(f"   æ€»æ“ä½œ: {len(results)}")
    print(f"   æˆåŠŸ: {successful}")
    print(f"   å¤±è´¥: {failed}")
    print(f"   è€—æ—¶: {end_time - start_time:.2f}s")
    print(f"   è¿æ¥æ± ç»Ÿè®¡: {pool.get_stats()}")

test_connection_pool_performance()
```

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### æ€§èƒ½åˆ†æå™¨

```python
import cProfile
import pstats
import io
from functools import wraps
import time

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.profiles = {}
        self.timing_data = {}
    
    def profile_function(self, func_name=None):
        """å‡½æ•°æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        def decorator(func):
            name = func_name or func.__name__
            
            @wraps(func)
            def wrapper(*args, **kwargs):
                # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
                profiler = cProfile.Profile()
                
                # å¼€å§‹åˆ†æ
                profiler.enable()
                start_time = time.time()
                
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    # åœæ­¢åˆ†æ
                    end_time = time.time()
                    profiler.disable()
                    
                    # ä¿å­˜åˆ†æç»“æœ
                    self._save_profile(name, profiler, end_time - start_time)
            
            return wrapper
        return decorator
    
    def _save_profile(self, name, profiler, execution_time):
        """ä¿å­˜åˆ†æç»“æœ"""
        # ä¿å­˜æ€§èƒ½åˆ†ææ•°æ®
        s = io.StringIO()
        ps = pstats.Stats(profiler, stream=s)
        ps.sort_stats('cumulative')
        ps.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªå‡½æ•°
        
        self.profiles[name] = s.getvalue()
        
        # ä¿å­˜æ—¶é—´æ•°æ®
        if name not in self.timing_data:
            self.timing_data[name] = []
        
        self.timing_data[name].append(execution_time)
    
    def get_profile_report(self, func_name):
        """è·å–æ€§èƒ½åˆ†ææŠ¥å‘Š"""
        if func_name in self.profiles:
            timing_data = self.timing_data.get(func_name, [])
            
            report = {
                'function_name': func_name,
                'call_count': len(timing_data),
                'total_time': sum(timing_data),
                'average_time': sum(timing_data) / len(timing_data) if timing_data else 0,
                'min_time': min(timing_data) if timing_data else 0,
                'max_time': max(timing_data) if timing_data else 0,
                'profile_details': self.profiles[func_name]
            }
            
            return report
        
        return None
    
    def get_summary_report(self):
        """è·å–æ±‡æ€»æŠ¥å‘Š"""
        summary = {}
        
        for func_name, timing_data in self.timing_data.items():
            summary[func_name] = {
                'call_count': len(timing_data),
                'total_time': sum(timing_data),
                'average_time': sum(timing_data) / len(timing_data),
                'min_time': min(timing_data),
                'max_time': max(timing_data)
            }
        
        return summary

# ä½¿ç”¨æ€§èƒ½åˆ†æå™¨
profiler = PerformanceProfiler()

@profiler.profile_function("tool_call_operation")
def optimized_tool_call(store, tool_name, arguments):
    """ä¼˜åŒ–çš„å·¥å…·è°ƒç”¨"""
    return store.call_tool(tool_name, arguments)

# æµ‹è¯•æ€§èƒ½åˆ†æ
for i in range(10):
    try:
        result = optimized_tool_call(store, "list_directory", {"path": "/tmp"})
    except:
        pass

# è·å–æ€§èƒ½æŠ¥å‘Š
report = profiler.get_profile_report("tool_call_operation")
if report:
    print(f"ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Š:")
    print(f"   å‡½æ•°: {report['function_name']}")
    print(f"   è°ƒç”¨æ¬¡æ•°: {report['call_count']}")
    print(f"   å¹³å‡è€—æ—¶: {report['average_time']:.4f}s")
    print(f"   æœ€å°è€—æ—¶: {report['min_time']:.4f}s")
    print(f"   æœ€å¤§è€—æ—¶: {report['max_time']:.4f}s")

# è·å–æ±‡æ€»æŠ¥å‘Š
summary = profiler.get_summary_report()
print(f"\nğŸ“ˆ æ€§èƒ½æ±‡æ€»:")
for func_name, stats in summary.items():
    print(f"   {func_name}: {stats['call_count']} æ¬¡è°ƒç”¨, å¹³å‡ {stats['average_time']:.4f}s")
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](performance.md)
- [ç›‘æ§ç³»ç»Ÿ](monitoring.md)
- [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](../architecture/overview.md)
- [é”™è¯¯å¤„ç†æœºåˆ¶](error-handling.md)

## ğŸ“š æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

1. **ç³»ç»Ÿçº§ä¼˜åŒ–**ï¼šåˆç†é…ç½®CPUã€å†…å­˜å’ŒI/Oèµ„æº
2. **åº”ç”¨çº§ä¼˜åŒ–**ï¼šä½¿ç”¨ç¼“å­˜ã€å¯¹è±¡æ± å’Œæ‰¹é‡æ“ä½œ
3. **ç½‘ç»œä¼˜åŒ–**ï¼šè¿æ¥æ± ã€åè®®ä¼˜åŒ–å’Œå¸¦å®½ç®¡ç†
4. **ç›‘æ§åˆ†æ**ï¼šæŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠæ—¶å‘ç°ç“¶é¢ˆ
5. **æ¸è¿›ä¼˜åŒ–**ï¼šä»æœ€å¤§çš„æ€§èƒ½ç“¶é¢ˆå¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–
6. **æµ‹è¯•éªŒè¯**ï¼šæ¯æ¬¡ä¼˜åŒ–åéƒ½è¦è¿›è¡Œæ€§èƒ½æµ‹è¯•éªŒè¯

---

**æ›´æ–°æ—¶é—´**: 2025-01-09  
**ç‰ˆæœ¬**: 1.0.0
